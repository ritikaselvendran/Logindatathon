{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. IMPORT PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn for machine learning and preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Processing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling and Hyperparameter tuning\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for model evaluation\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ritik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ritik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ritik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download required NLTK resources\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DATA UNDERSTANDING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Understanding\n",
    "\n",
    "The provided datasets contain information about various entities, including metadata and textual descriptions. Here's an overview of both the **Train** and **Test** datasets:\n",
    "\n",
    "#### **Train Dataset**:\n",
    "- **Number of rows**: 100,000\n",
    "- **Columns**:\n",
    "  1. **ENTITY_ID**: (int64) Unique identifier for each entity.\n",
    "  2. **CATEGORY_ID**: (int64) Represents the category to which the entity belongs.\n",
    "  3. **ENTITY_LENGTH**: (float64) The target variable, representing the length of the entity. This is what we are trying to predict.\n",
    "  4. **ENTITY_DESCRIPTION**: (object) A textual description of the entity, potentially containing key information that may help in predicting the entity's length.\n",
    "\n",
    "#### **Test Dataset**:\n",
    "- **Number of rows**: 20,000\n",
    "- **Columns**:\n",
    "  1. **ENTITY_ID**: (int64) Unique identifier for each entity.\n",
    "  2. **CATEGORY_ID**: (int64) Represents the category to which the entity belongs.\n",
    "  3. **ENTITY_DESCRIPTION**: (object) A textual description of the entity, similar to the train dataset.\n",
    "\n",
    "### Observations:\n",
    "- **Train Data**: The train dataset includes both the **ENTITY_LENGTH** (target variable) and features such as the **CATEGORY_ID** and **ENTITY_DESCRIPTION**. The latter contains important textual information that could be relevant for predicting the length of the entity.\n",
    "- **Test Data**: The test dataset is similar to the train dataset but without the **ENTITY_LENGTH**. This will be used for testing the model's predictions.\n",
    "\n",
    "### Next Steps:\n",
    "1. **Preprocessing**: Clean and preprocess the text in the **ENTITY_DESCRIPTION** column.\n",
    "2. **Feature Engineering**: Extract useful features from **ENTITY_DESCRIPTION** and combine them with **CATEGORY_ID**.\n",
    "3. **Model Building**: Develop machine learning models to predict **ENTITY_LENGTH** using the provided features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 4 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   ENTITY_ID           100000 non-null  int64  \n",
      " 1   CATEGORY_ID         100000 non-null  int64  \n",
      " 2   ENTITY_LENGTH       100000 non-null  float64\n",
      " 3   ENTITY_DESCRIPTION  100000 non-null  object \n",
      "dtypes: float64(1), int64(2), object(1)\n",
      "memory usage: 3.1+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 3 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   ENTITY_ID           20000 non-null  int64 \n",
      " 1   CATEGORY_ID         20000 non-null  int64 \n",
      " 2   ENTITY_DESCRIPTION  20000 non-null  object\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 468.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       "    ENTITY_ID  CATEGORY_ID  ENTITY_LENGTH  \\\n",
       " 0     216064          112     600.000000   \n",
       " 1    2498090         8360     984.251967   \n",
       " 2     210860           30     850.000000   \n",
       " 3     285757         6104     744.000000   \n",
       " 4    1869643         2201     600.000000   \n",
       " \n",
       "                                   ENTITY_DESCRIPTION  \n",
       " 0                               Caricaturas (1892)    \n",
       " 1  VJ Interior Modern Solid Interio Cushioned Bar...  \n",
       " 2  Pam: Personalized Name Journal with Blank Line...  \n",
       " 3  Hio Nya Testamenti Drottins Vors Jesu Krists A...  \n",
       " 4  ADDIT Phone Case for Samsung Galaxy A50 [ Mili...  )"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the datasets\n",
    "train_df = pd.read_csv('Train.csv')\n",
    "test_df = pd.read_csv('Test.csv')\n",
    "\n",
    "# Display basic information and structure of the data\n",
    "train_info = train_df.info()\n",
    "test_info = test_df.info()\n",
    "\n",
    "# Display the first few rows of the datasets to understand the content\n",
    "train_head = train_df.head()\n",
    "test_head = test_df.head()\n",
    "\n",
    "(train_info,  train_head)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PRE PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100000, 500),\n",
       "     10  100   11   12   13   14       15   16        18   20  ...  women  \\\n",
       " 0  0.0  0.0  0.0  0.0  0.0  0.0  0.00000  0.0  0.000000  0.0  ...    0.0   \n",
       " 1  0.0  0.0  0.0  0.0  0.0  0.0  0.16158  0.0  0.491231  0.0  ...    0.0   \n",
       " 2  0.0  0.0  0.0  0.0  0.0  0.0  0.00000  0.0  0.000000  0.0  ...    0.0   \n",
       " 3  0.0  0.0  0.0  0.0  0.0  0.0  0.00000  0.0  0.000000  0.0  ...    0.0   \n",
       " 4  0.0  0.0  0.0  0.0  0.0  0.0  0.00000  0.0  0.000000  0.0  ...    0.0   \n",
       " \n",
       "    womens  wood  wooden  work  world  year  years    yellow  zipper  \n",
       " 0     0.0   0.0     0.0   0.0    0.0   0.0    0.0  0.000000     0.0  \n",
       " 1     0.0   0.0     0.0   0.0    0.0   0.0    0.0  0.087055     0.0  \n",
       " 2     0.0   0.0     0.0   0.0    0.0   0.0    0.0  0.000000     0.0  \n",
       " 3     0.0   0.0     0.0   0.0    0.0   0.0    0.0  0.000000     0.0  \n",
       " 4     0.0   0.0     0.0   0.0    0.0   0.0    0.0  0.000000     0.0  \n",
       " \n",
       " [5 rows x 500 columns])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually defining a basic set of stopwords for English\n",
    "custom_stopwords = [\n",
    "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your',\n",
    "    'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it',\n",
    "    \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this',\n",
    "    'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has',\n",
    "    'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "    'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n",
    "    'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\n",
    "    'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most',\n",
    "    'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can',\n",
    "    'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren',\n",
    "    \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven',\n",
    "    \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\",\n",
    "    'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"\n",
    "]\n",
    "\n",
    "# Initialize TF-IDF Vectorizer with the custom stopword list\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=500, stop_words=custom_stopwords)\n",
    "\n",
    "# Apply TF-IDF Vectorizer on 'ENTITY_DESCRIPTION'\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(train_df['ENTITY_DESCRIPTION'])\n",
    "\n",
    "# Convert to a DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Check the shape and first few columns of the transformed text data\n",
    "tfidf_df.shape, tfidf_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=500, stop_words='english')\n",
    "\n",
    "# Apply TF-IDF Vectorizer to 'ENTITY_DESCRIPTION' in the training data\n",
    "train_tfidf_matrix = tfidf_vectorizer.fit_transform(train_df['ENTITY_DESCRIPTION'])\n",
    "\n",
    "# Convert TF-IDF matrix to a DataFrame for train data\n",
    "train_tfidf_df = pd.DataFrame(train_tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Apply TF-IDF Vectorizer to 'ENTITY_DESCRIPTION' in the test data\n",
    "test_tfidf_matrix = tfidf_vectorizer.transform(test_df['ENTITY_DESCRIPTION'])\n",
    "\n",
    "# Convert TF-IDF matrix to a DataFrame for test data\n",
    "test_tfidf_df = pd.DataFrame(test_tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COMBING TEXT FEATURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine 'CATEGORY_ID' with TF-IDF features for training data\n",
    "train_combined = pd.concat([train_df[['CATEGORY_ID']], train_tfidf_df], axis=1)\n",
    "\n",
    "# Add the target variable 'ENTITY_LENGTH' to the training set\n",
    "train_combined['ENTITY_LENGTH'] = train_df['ENTITY_LENGTH']\n",
    "\n",
    "# Combine 'CATEGORY_ID' with TF-IDF features for test data\n",
    "test_combined = pd.concat([test_df[['CATEGORY_ID']], test_tfidf_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. MODEL BUILDING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RANDOMFORREST REGRESSION MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForestRegressor is an ensemble learning method used for regression tasks. It is based on the idea of combining multiple decision trees (a forest) to improve the accuracy and stability of predictions. Each decision tree in the forest is trained on a different random subset of the data, which helps in reducing overfitting and variance in the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define features (X) and target (y) for training\n",
    "X_train = train_combined.drop(columns=['ENTITY_LENGTH'])\n",
    "y_train = train_combined['ENTITY_LENGTH']\n",
    "\n",
    "# Split training data into train and validation sets\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(n_estimators=100,max_depth=10, random_state=42)\n",
    "rf_model.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Predictions for validation set\n",
    "y_val_pred = rf_model.predict(X_val_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAPE: 2.6462734499083003\n",
      "Validation RMSE: 655.4848322969141\n",
      "Validation R²: 0.036157617843226464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ritik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, r2_score\n",
    "\n",
    "# Evaluate on validation set\n",
    "mape_val = mean_absolute_percentage_error(y_val_split, y_val_pred)\n",
    "rmse_val = mean_squared_error(y_val_split, y_val_pred, squared=False)\n",
    "r2_val = r2_score(y_val_split, y_val_pred)\n",
    "\n",
    "print('Validation MAPE:', mape_val)\n",
    "print('Validation RMSE:', rmse_val)\n",
    "print('Validation R²:', r2_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'ENTITY_ID': test_df['ENTITY_ID'],  # Assuming 'ENTITY_ID' is present in the test data\n",
    "    'ENTITY_LENGTH': y_test_pred\n",
    "})\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "predictions_df.to_csv('sample_submissions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
